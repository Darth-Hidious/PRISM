# prism run — Autonomous Agent Mode

Run PRISM's AI agent autonomously on a research goal. The agent uses a
Think-Act-Observe-Repeat (TAOR) loop to break down the goal, call tools,
and synthesize a final answer.

`prism run` is the primary command for interacting with PRISM. It replaces
the deprecated `prism ask` command (which now redirects to `run`).

## Usage

```bash
prism run "your research goal here"
prism run "find stable perovskites for solar cells" --confirm
prism run "compare band gaps across OMAT24 and MP" --model gpt-4.1
prism run "plan DFT relaxations for Fe-Ni alloys" --agent calphad_expert
```

## Options

| Flag | Description |
|------|-------------|
| `--agent NAME` | Use a named agent config from the plugin registry |
| `--provider NAME` | LLM provider: `anthropic`, `openai`, `openrouter`, `marc27` |
| `--model NAME` | Model override (e.g. `claude-sonnet-4-6`, `gpt-4.1`, `glm-4.7`) |
| `--confirm` | Require user confirmation before expensive tool calls |
| `--dangerously-accept-all` | Auto-approve all tool calls without prompting |
| `--no-mcp` | Disable loading tools from external MCP servers (parent flag) |

## Related Commands

| Command | Purpose |
|---------|---------|
| `prism run "goal"` | Autonomous agent — plans, calls tools, synthesizes answer |
| `prism search --elements Fe,Ni` | Structured OPTIMADE search (no LLM, direct query) |
| `prism ask "query"` | **Deprecated** — redirects to `prism run` |
| `prism` (no args) | Interactive REPL session |

`prism search` is a direct CLI for federated OPTIMADE queries (elements,
formula, band gap, space group filters). It does not use an LLM. Use it
when you know exactly what filter you want. `prism run` is the agent-driven
alternative — give it a goal in natural language and it figures out which
tools and filters to use.

---

## How It Works

### TAOR Loop

1. **Goal input** — your natural language goal is sent to the LLM.
2. **Planning** — for complex goals, the agent outputs a `<plan>...</plan>` block
   listing numbered steps before executing anything.
3. **TAOR loop** — the agent repeatedly:
   - **Think**: reason about what to do next (displayed as text above tool panels)
   - **Act**: call a tool with arguments
   - **Observe**: read the tool result
   - **Repeat**: continue until the goal is answered
4. **Synthesis** — the agent produces a final Markdown answer with citations.

The loop runs up to 30 iterations by default.

### Display Output

The `run` command streams output with persistent display — nothing gets wiped:

```
╭───────────────────────────────────────────────────────╮
│ Goal: Find iron oxide structures and their space groups │
╰───────────────────────────────────────────────────────╯
I'll search for iron oxide structures. Let me query the          ← reasoning (persists)
federated materials databases.
╭─ search_materials ─╮
│ Calling...         │                                           ← tool call panel
╰────────────────────╯
╭────── search_materials ──────╮
│ search_materials: 50 results │                                 ← tool result panel
╰──────────────────────────────╯
Let me also check the literature for crystal structure data.     ← more reasoning (persists)
╭─ literature_search ─╮
│ Calling...           │
╰──────────────────────╯
╭────── literature_search ──────╮
│ literature_search: 10 results │
╰───────────────────────────────╯

## 3 Iron Oxide Structures                                       ← final answer (Markdown)

1. **Wüstite (FeO)** — Space Group: Fm-3m (cubic)
2. **Hematite (Fe₂O₃)** — Space Group: R-3c (trigonal)
3. **Magnetite (Fe₃O₄)** — Space Group: Fd-3m (cubic)

tokens: 26,716in + 1,098out | cost: $0.1070                     ← usage/cost footer
```

### Output Events

The stream produces four event types, all displayed persistently:

| Event | Display | Description |
|-------|---------|-------------|
| `TextDelta` | Markdown text | LLM reasoning or final answer, streamed token-by-token |
| `ToolCallStart` | Yellow panel | Tool name, shown when the agent begins a tool call |
| `ToolCallResult` | Green panel | Tool name + one-line summary of result |
| `TurnComplete` | Cost footer | Marks end of run with token/cost summary |

---

## Token and Cost Tracking

Every run tracks token usage and estimated cost. The final line of output shows:

```
tokens: 26,716in + 1,098out | cost: $0.1070
```

### UsageInfo Fields

Each API call extracts a `UsageInfo` record with:

| Field | Description |
|-------|-------------|
| `input_tokens` | Tokens sent to the model (prompt + history + tool results) |
| `output_tokens` | Tokens generated by the model (reasoning + tool calls + answer) |
| `cache_creation_tokens` | Tokens written to Anthropic prompt cache (first turn) |
| `cache_read_tokens` | Tokens read from cache (subsequent turns, 90% cheaper) |
| `total_tokens` | `input_tokens + output_tokens` |

### TurnComplete Fields

The `TurnComplete` event at end-of-run carries:

| Field | Type | Description |
|-------|------|-------------|
| `text` | `str` | Final answer text |
| `usage` | `UsageInfo` | Token usage for this turn only |
| `total_usage` | `UsageInfo` | Cumulative tokens across all TAOR iterations |
| `estimated_cost` | `float` | Cumulative USD estimate based on model pricing |

### Cost Calculation

Cost is computed from the model's pricing in the ModelConfig registry:

```
cost = (input_tokens * input_price_per_mtok / 1,000,000)
     + (output_tokens * output_price_per_mtok / 1,000,000)
     + (cache_read_tokens * input_price_per_mtok * 0.1 / 1,000,000)
```

Cache reads are billed at 10% of the input token rate (Anthropic pricing).

### TUI Integration Points

The `TurnComplete` event and `UsageInfo` dataclass are the integration
surface for future TUI billing display:

- **Session cost widget**: accumulate `estimated_cost` across runs in a session
- **Token budget bar**: compare `total_usage.total_tokens` against model's `context_window`
- **Per-turn breakdown**: each intermediate TAOR iteration has its own `usage`
- **Cache hit rate**: `cache_read_tokens / (input_tokens + cache_read_tokens)`
- **Cost-per-query history**: log `estimated_cost` per run for billing dashboard
- **Provider cost comparison**: same query across providers using model registry pricing

These fields are already emitted by `AgentCore` — the TUI just needs to
subscribe to `TurnComplete` events and render the data.

---

## Provider Auto-Detection

If `--provider` is not specified, PRISM checks environment variables in order:

1. `MARC27_TOKEN` or `~/.prism/marc27_token` — MARC27 managed backend
2. `ANTHROPIC_API_KEY` — Anthropic (Claude models)
3. `OPENAI_API_KEY` — OpenAI (GPT/o3 models)
4. `OPENROUTER_API_KEY` — OpenRouter (multi-provider gateway)

Run `prism setup` to configure keys interactively.

---

## LLM Connection Layer

The agent backend includes six production-grade features for resilience,
cost control, and observability.

### Model Config Registry

Every supported model has a frozen `ModelConfig` with context window size,
max output tokens, pricing, and capability flags (caching, thinking, tools).
The agent uses model-aware `max_tokens` instead of a hardcoded value.

**18 models across 4 providers:**

| Provider | Models | Context | Default max_tokens |
|----------|--------|---------|-------------------|
| Anthropic | claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5 | 200K | 8K–32K |
| OpenAI | gpt-4o, gpt-4.1, gpt-5, o3, o3-mini | 128K–1M | 4K–16K |
| Google | gemini-2.5-pro, gemini-2.5-flash, gemini-3.1-pro | 1M | 8K–16K |
| Zhipu | glm-5, glm-4.7, glm-4.5-air | 128K–200K | 4K–16K |

Unknown models get conservative defaults (128K context, 8K max_tokens).
OpenRouter-prefixed IDs (e.g. `anthropic/claude-opus-4-6`) are stripped
automatically.

### Prompt Caching

Anthropic system prompts are wrapped with `cache_control: {"type": "ephemeral"}`,
enabling the API to cache and reuse the system prompt across turns. Cache reads
are 90% cheaper than re-processing. OpenAI does server-side caching automatically.

### Retry with Exponential Backoff

Transient API errors (HTTP 429, 500, 502, 503) are retried up to 3 times
with exponential backoff: 1s, 2s, 4s (capped at 8s). The `Retry-After`
header is respected when present. Auth errors (401) and bad requests (400)
are never retried.

### Large Result Handling (RLM-Inspired ResultStore)

When a tool result exceeds 30,000 characters, the full result is stored
in an in-memory `ResultStore` and the agent receives:
- A 2,000-character preview
- A `result_id` for use with the `peek_result` tool
- Instructions to page through the full result

The agent can then call `peek_result(result_id="<id>", offset=0, limit=5000)`
to read specific sections, or use `export_results_csv` to save the full
result to a file.

This follows the RLM paradigm (Zhang et al., "Recursive Language Models",
MIT CSAIL, 2025) — treating large inputs as external environment variables
the agent can programmatically access rather than cramming them into the
context window.

### Doom Loop Detection

The agent tracks the last 10 tool calls. If the same tool with the same
arguments fails 3 times consecutively, a system warning is injected:

> DOOM LOOP DETECTED: tool_name has failed 3 times with the same arguments.
> Try a different approach, different arguments, or ask the user for help.

The agent sees this warning and changes strategy.

---

## Available Tools

The `run` command has access to all registered tools and skills:

**Materials data:**
- `search_materials` — federated OPTIMADE search across 30+ providers (fusion, dedup, ranking)
- `query_materials_project` — Materials Project native API (requires MP_API_KEY)
- `export_results_csv` — save tabular results to CSV

**Literature and patents** (separate tools, not part of `search_materials`):
- `literature_search` — arXiv + Semantic Scholar paper search
- `patent_search` — Lens.org patent search (requires LENS_API_TOKEN)

**Analysis:**
- `predict_property` / `predict_properties` — ML property prediction
- `calculate_phase_diagram`, `calculate_equilibrium` — CALPHAD thermodynamics
- `validate_dataset`, `review_dataset` — data quality checks

**Visualization:**
- `plot_materials_comparison`, `plot_correlation_matrix`, `plot_property_distribution`

**Multi-step skills** (orchestrate multiple tools):
- `materials_discovery` — end-to-end pipeline (acquire, predict, visualize, report)
- `acquire_materials` — collect data from multiple sources
- `plan_simulations` — auto-routes CALPHAD vs DFT vs MD
- `analyze_phases` — phase stability analysis using CALPHAD
- `generate_report` — Markdown/HTML/PDF reports with correlations
- `select_materials` — filter and rank candidates by criteria
- `visualize_dataset` — generate plots for dataset columns

**System:**
- `read_file`, `write_file` — local filesystem access
- `web_search` — general web search
- `peek_result` — page through stored large results (auto-registered)

### Plugins and Custom Skills

Tools and skills are loaded from the plugin registry (`app/plugins/catalog.json`).
Users can add their own tools and skills as plugins — each plugin provides
tools the agent can call and/or skills that orchestrate multi-step workflows.

The workflow shown here (search, predict, simulate, validate, report) is one
approach to materials discovery. Users can build entirely different workflows
by writing plugins for their specific methodology — e.g., high-throughput
screening, active learning loops, or experimental design optimization.

See `prism plugin list` to view installed plugins.

---

## Search Fusion and Provider Separation

The `search_materials` tool queries 30+ OPTIMADE providers in parallel and
fuses results by identity key (`formula::space_group`). Provider provenance
is fully preserved:

- `Material.sources: list[str]` — which providers contributed this record
- `PropertyValue.source: str` — which provider supplied each property value
- Conflicting values are stored in `extra_properties` with source-tagged keys
  (e.g. `band_gap:cod`, `band_gap:mp`)

This means you can:

1. **Collect enriched data** — fuse properties across MP, COD, AFLOW, JARVIS, etc.
2. **Separate by provider** — filter by `Material.sources` to get provider-specific
   subsets for training DFT surrogates or ML models.
3. **Benchmark models** — train on one provider's data, test against another's.
4. **Compare values** — examine how the same property differs across providers.

```python
# Example: separate MP data for training, COD for benchmarking
from app.search.fusion import fuse_materials

fused = fuse_materials(all_results)
mp_only = [m for m in fused if "mp" in m.sources]
cod_only = [m for m in fused if "cod" in m.sources]
```

Or pre-filter at query time:

```python
from app.search.query import MaterialSearchQuery

query = MaterialSearchQuery(elements=["Fe", "Ni"], providers=["mp"])
```
